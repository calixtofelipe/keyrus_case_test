{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "liked-packing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dirty-correlation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark import SparkFiles\n",
    "import zipfile\n",
    "import io\n",
    "from config import FTP_DATASOURCE, FTP_DATASOURCE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "instant-field",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inicialize session and get context spark\n",
    "spark = SparkSession.builder.appName(\"ftp_spark\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "necessary-being",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to extract and save database path\n",
    "def zip_extract_save(x):\n",
    "    in_memory_data = io.BytesIO(x[1])\n",
    "    with zipfile.ZipFile(in_memory_data, \"r\") as zipped:\n",
    "            zipped.extractall(\"database\")\n",
    "    \n",
    "zips = sc.binaryFiles(FTP_DATASOURCE_URI)\n",
    "files_data = zips.map(zip_extract_save)\n",
    "files_data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "derived-worst",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function unzip to rdd\n",
    "def zip_extract_dataframe_panda(x):\n",
    "    in_memory_data = io.BytesIO(x[1])\n",
    "    \n",
    "    zf = zipfile.ZipFile(in_memory_data)\n",
    "    f = zf.read('data_municipio.csv').decode('UTF-8')\n",
    "    seq = 0\n",
    "    all_lines = []\n",
    "    for line in f.split('\\r'):\n",
    "        seq+=1\n",
    "        if(seq>1 and line !='\\n'):\n",
    "            l = line.split(';')\n",
    "            all_lines.append(tuple(l[0:4]))\n",
    "    return all_lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sacred-sender",
   "metadata": {},
   "outputs": [],
   "source": [
    "#consumindo ftp e get arquivo bin√°rio\n",
    "zips = sc.binaryFiles(FTP_DATASOURCE_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "golden-landscape",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracao e transformacao em rdd\n",
    "files_data = zips.map(zip_extract_dataframe_panda)\n",
    "schema = StructType([ \\\n",
    "    StructField(\"transacao\",StringType(),True), \\\n",
    "    StructField(\"municipio\",StringType(),True), \\\n",
    "    StructField(\"estado\",StringType(),True), \\\n",
    "    StructField(\"data_atualizacao\", StringType(), True),\n",
    "  ])\n",
    "#criando dataframe com o rdd e schema acima\n",
    "df = spark.createDataFrame(files_data.collect()[0],schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "assumed-celtic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+------+----------------+\n",
      "|transacao|     municipio|estado|data_atualizacao|\n",
      "+---------+--------------+------+----------------+\n",
      "|       10|    UBERLANDIA|    MG|      23/02/2021|\n",
      "|       50|     SAO PAULO|    SP|      24/02/2021|\n",
      "|        5|     SAO PAULO|    SP|      25/02/2021|\n",
      "|       20|    UBERLANDIA|    MG|      23/02/2021|\n",
      "|      100|RIO DE JANEIRO|    RJ|      23/02/2021|\n",
      "|    200,2|RIO DE JANEIRO|    RJ|      24/02/2021|\n",
      "|    50,41|      CAMPINAS|    SP|      24/02/2021|\n",
      "|       30|      CAMPINAS|    SP|      24/02/2021|\n",
      "|      500|     SAO PAULO|    SP|      25/02/2021|\n",
      "+---------+--------------+------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "incorporated-enforcement",
   "metadata": {},
   "outputs": [],
   "source": [
    "#excrita em parquet\n",
    "df.write.format('parquet').save('database/data_municipio_zip_2_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sporting-drill",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lendo arquivo que foi salvo e reescrevendo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "serious-stevens",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('database/data_municipio.csv', sep=';',header=True, ignoreTrailingWhiteSpace=True)\n",
    "df = df.drop('_c4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "featured-rider",
   "metadata": {},
   "outputs": [],
   "source": [
    "# savando como parquet0\n",
    "df.repartition(8).write.format('parquet').save('database/data_municipio.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
