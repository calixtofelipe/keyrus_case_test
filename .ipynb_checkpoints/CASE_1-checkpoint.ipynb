{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "expressed-bench",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cross-treat",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark import SparkFiles\n",
    "import zipfile\n",
    "import io\n",
    "from config import FTP_DATASOURCE, FTP_DATASOURCE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "imposed-shape",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inicialize session and get context spark\n",
    "spark = SparkSession.builder.appName(\"teste\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "classical-annex",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to extract and save database path\n",
    "def zip_extract_save(x):\n",
    "    in_memory_data = io.BytesIO(x[1])\n",
    "    with zipfile.ZipFile(in_memory_data, \"r\") as zipped:\n",
    "            zipped.extractall(\"database\")\n",
    "    \n",
    "zips = sc.binaryFiles(FTP_DATASOURCE_URI)\n",
    "files_data = zips.map(zip_extract_save)\n",
    "files_data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "maritime-length",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[  TRANSACAO       MUNICIPIO ESTADO DATA_ATUALIZACAO  Unnamed: 4\n",
       " 0        10      UBERLANDIA     MG       23/02/2021         NaN\n",
       " 1        50       SAO PAULO     SP       24/02/2021         NaN\n",
       " 2         5       SAO PAULO     SP       25/02/2021         NaN\n",
       " 3        20      UBERLANDIA     MG       23/02/2021         NaN\n",
       " 4       100  RIO DE JANEIRO     RJ       23/02/2021         NaN\n",
       " 5     200,2  RIO DE JANEIRO     RJ       24/02/2021         NaN\n",
       " 6     50,41        CAMPINAS     SP       24/02/2021         NaN\n",
       " 7        30        CAMPINAS     SP       24/02/2021         NaN\n",
       " 8       500       SAO PAULO     SP       25/02/2021         NaN]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#function unzip save dataframe pandas\n",
    "def zip_extract_dataframe_panda(x):\n",
    "    in_memory_data = io.BytesIO(x[1])\n",
    "    file_obj = zipfile.ZipFile(in_memory_data, \"r\")\n",
    "    #files = [i for i in file_obj.namelist()]\n",
    "    for file in file_obj.namelist():\n",
    "        f = file_obj.open(file)\n",
    "        df = pd.read_csv(f, sep=';')\n",
    "        #dict(zip(files, [file_obj.open(file).read() for file in files]))        \n",
    "    return df\n",
    "\n",
    "\n",
    "zips = sc.binaryFiles(FTP_DATASOURCE_URI)\n",
    "files_data = zips.map(zip_extract)\n",
    "files_data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proprietary-recognition",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "automatic-venture",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-114-84e2357dedf5>, line 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-114-84e2357dedf5>\"\u001b[0;36m, line \u001b[0;32m15\u001b[0m\n\u001b[0;31m    return 'teste'\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "#function unzip save dataframe pandas\n",
    "def zip_extract_dataframe_panda(x):\n",
    "    in_memory_data = io.BytesIO(x[1])\n",
    "    with zipfile(in_memory_data) as myzip:\n",
    "        with myzip.open('data_municipio.csv') as myfile:\n",
    "            \n",
    "    #file_obj = zipfile.ZipFile(in_memory_data, \"r\")\n",
    "    ##files = [i for i in file_obj.namelist()]\n",
    "    #for file in file_obj.namelist():\n",
    "    #    f = file_obj.open(file)\n",
    "     #   f.seek(0)\n",
    "      #  df = f.readline()\n",
    "       # break\n",
    "        #dict(zip(files, [file_obj.open(file).read() for file in files]))        \n",
    "    return 'teste'\n",
    "\n",
    "\n",
    "zips = sc.binaryFiles(FTP_DATASOURCE_URI)\n",
    "files_data = zips.map(zip_extract_dataframe_panda)\n",
    "files_data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "terminal-valve",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method createDataFrame in module pyspark.sql.session:\n",
      "\n",
      "createDataFrame(data, schema=None, samplingRatio=None, verifySchema=True) method of pyspark.sql.session.SparkSession instance\n",
      "    Creates a :class:`DataFrame` from an :class:`RDD`, a list or a :class:`pandas.DataFrame`.\n",
      "    \n",
      "    When ``schema`` is a list of column names, the type of each column\n",
      "    will be inferred from ``data``.\n",
      "    \n",
      "    When ``schema`` is ``None``, it will try to infer the schema (column names and types)\n",
      "    from ``data``, which should be an RDD of either :class:`Row`,\n",
      "    :class:`namedtuple`, or :class:`dict`.\n",
      "    \n",
      "    When ``schema`` is :class:`pyspark.sql.types.DataType` or a datatype string, it must match\n",
      "    the real data, or an exception will be thrown at runtime. If the given schema is not\n",
      "    :class:`pyspark.sql.types.StructType`, it will be wrapped into a\n",
      "    :class:`pyspark.sql.types.StructType` as its only field, and the field name will be \"value\".\n",
      "    Each record will also be wrapped into a tuple, which can be converted to row later.\n",
      "    \n",
      "    If schema inference is needed, ``samplingRatio`` is used to determined the ratio of\n",
      "    rows used for schema inference. The first row will be used if ``samplingRatio`` is ``None``.\n",
      "    \n",
      "    :param data: an RDD of any kind of SQL data representation (e.g. row, tuple, int, boolean,\n",
      "        etc.), :class:`list`, or :class:`pandas.DataFrame`.\n",
      "    :param schema: a :class:`pyspark.sql.types.DataType` or a datatype string or a list of\n",
      "        column names, default is ``None``.  The data type string format equals to\n",
      "        :class:`pyspark.sql.types.DataType.simpleString`, except that top level struct type can\n",
      "        omit the ``struct<>`` and atomic types use ``typeName()`` as their format, e.g. use\n",
      "        ``byte`` instead of ``tinyint`` for :class:`pyspark.sql.types.ByteType`. We can also use\n",
      "        ``int`` as a short name for ``IntegerType``.\n",
      "    :param samplingRatio: the sample ratio of rows used for inferring\n",
      "    :param verifySchema: verify data types of every row against schema.\n",
      "    :return: :class:`DataFrame`\n",
      "    \n",
      "    .. versionchanged:: 2.1\n",
      "       Added verifySchema.\n",
      "    \n",
      "    .. note:: Usage with spark.sql.execution.arrow.pyspark.enabled=True is experimental.\n",
      "    \n",
      "    .. note:: When Arrow optimization is enabled, strings inside Pandas DataFrame in Python\n",
      "        2 are converted into bytes as they are bytes in Python 2 whereas regular strings are\n",
      "        left as strings. When using strings in Python 2, use unicode `u\"\"` as Python standard\n",
      "        practice.\n",
      "    \n",
      "    >>> l = [('Alice', 1)]\n",
      "    >>> spark.createDataFrame(l).collect()\n",
      "    [Row(_1='Alice', _2=1)]\n",
      "    >>> spark.createDataFrame(l, ['name', 'age']).collect()\n",
      "    [Row(name='Alice', age=1)]\n",
      "    \n",
      "    >>> d = [{'name': 'Alice', 'age': 1}]\n",
      "    >>> spark.createDataFrame(d).collect()\n",
      "    [Row(age=1, name='Alice')]\n",
      "    \n",
      "    >>> rdd = sc.parallelize(l)\n",
      "    >>> spark.createDataFrame(rdd).collect()\n",
      "    [Row(_1='Alice', _2=1)]\n",
      "    >>> df = spark.createDataFrame(rdd, ['name', 'age'])\n",
      "    >>> df.collect()\n",
      "    [Row(name='Alice', age=1)]\n",
      "    \n",
      "    >>> from pyspark.sql import Row\n",
      "    >>> Person = Row('name', 'age')\n",
      "    >>> person = rdd.map(lambda r: Person(*r))\n",
      "    >>> df2 = spark.createDataFrame(person)\n",
      "    >>> df2.collect()\n",
      "    [Row(name='Alice', age=1)]\n",
      "    \n",
      "    >>> from pyspark.sql.types import *\n",
      "    >>> schema = StructType([\n",
      "    ...    StructField(\"name\", StringType(), True),\n",
      "    ...    StructField(\"age\", IntegerType(), True)])\n",
      "    >>> df3 = spark.createDataFrame(rdd, schema)\n",
      "    >>> df3.collect()\n",
      "    [Row(name='Alice', age=1)]\n",
      "    \n",
      "    >>> spark.createDataFrame(df.toPandas()).collect()  # doctest: +SKIP\n",
      "    [Row(name='Alice', age=1)]\n",
      "    >>> spark.createDataFrame(pandas.DataFrame([[1, 2]])).collect()  # doctest: +SKIP\n",
      "    [Row(0=1, 1=2)]\n",
      "    \n",
      "    >>> spark.createDataFrame(rdd, \"a: string, b: int\").collect()\n",
      "    [Row(a='Alice', b=1)]\n",
      "    >>> rdd = rdd.map(lambda row: row[1])\n",
      "    >>> spark.createDataFrame(rdd, \"int\").collect()\n",
      "    [Row(value=1)]\n",
      "    >>> spark.createDataFrame(rdd, \"boolean\").collect() # doctest: +IGNORE_EXCEPTION_DETAIL\n",
      "    Traceback (most recent call last):\n",
      "        ...\n",
      "    Py4JJavaError: ...\n",
      "    \n",
      "    .. versionadded:: 2.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(spark.createDataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incredible-samba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lendo arquivo que foi salvo e reescrevendo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "saved-mustang",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('database/data_municipio.csv', sep=';',header=True, ignoreTrailingWhiteSpace=True)\n",
    "df = df.drop('_c4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "silent-canyon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# savando como parquet0\n",
    "df.repartition(8).write.format('parquet').save('database/data_municipio.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
